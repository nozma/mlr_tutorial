# ベンチマーク試験

```{r include=FALSE}
library(mlr)
```


ベンチマーク試験では、1つ、あるいは複数の性能指標に基いてアルゴリズムを比較するために、異なる学習手法を1つあるいは複数のデータセットに適用する。

`mlr`では`benchmark`関数に学習器とタスクをリストで渡すことでベンチマーク試験を実施できる。`benchmark`は通常、学習器とタスクの対に対してリサンプリングを実行する。タスクと性能指標の組み合わせに対してどのようなリサンプリング手法を適用するかは選択することができる。

## ベンチマーク試験の実施

小さな例から始めよう。線形判別分析(lda)と分類木(rpart)を`sonar.task`に適用する。リサンプリング手法はホールドアウト法を用いる。

以下の例では`ResampleDesc`オブジェクトを作成する。各リサンプリングのインスタンスは`benchmark`関数によって自動的に作成される。インスタンス化はタスクに対して1度だけ実行される。つまり、全ての学習器は全くおなじ訓練セット、テストセットを用いることになる。なお、明示的に`ResampleInstance`を渡しても良い。

もし、データセットの作成を無作為ではなく任意に行いたければ、`makeFixedHoldoutinstance`を使うと良いだろう。

```{r}
lrns = list(makeLearner("classif.lda"), makeLearner("classif.rpart"))

rdesc = makeResampleDesc("Holdout")

bmr = benchmark(lrns, sonar.task, rdesc)
```

```{r}
bmr
```

もし`makeLearner`に学習器の種類以外の引数を指定するつもりがなければ、明示的に`makeLearner`を呼び出さずに単に学習器の名前を指定しても良い。上記の例は次のように書き換えることができる。

```{r}
## 学習器の名前だけをベクトルで指定しても良い
lrns = c("classif.lda", "classif.rpart")

## 学習器の名前とLearnerオブジェクトを混ぜたリストでも良い
lrns = list(makeLearner("classif.lda", predict.type = "prob"), "classif.rpart")

bmr = benchmark(lrns, sonar.task, rdesc)
```

```{r}
bmr
```

`print`の結果は各行がタスクと学習器の1つの組合せに対応している。ここでは分類のデフォルトの指標である平均誤分類率が示されている。

`bmr`は`BenchmarcResult`クラスのオブジェクトである。基本的には、これは`ResampleResult`クラスのオブジェクトのリストのリストを含んでおり、最初のリストはタスク、その中のリストは学習器に対応した並びになっている。

### 実験を再現可能にする

一般的にいって、実験は再現可能であることが望ましい。`mlr`は`set.seed`関数の設定に従うので、スクリプト実行前に`set.seed`によって乱数種を固定しておけば再現性が確保できる。

もし並列計算を使用する場合は、ユースケースに合わせて`set.seed`の呼び出し方を調整する必要がある。例えば、`set.seed(123, "L'Ecuyer")`と指定すると子プロセス単位で再現性が確保できる。mcapplyの例([mclapply function | R Documentation](https://www.rdocumentation.org/packages/parallel/versions/3.4.1/topics/mclapply))を見ると並列計算と再現性に関するもう少し詳しい情報が得られるだろう(訳注:こちらのほうが良いかも？[R: Parallel version of lapply](https://rforge.net/doc/packages/multicore/mclapply.html))。

## ベンチマーク結果へのアクセス

`mlr`は`getBMR<抽出対象>`という名前のアクセサ関数を幾つか用意している。これにより、さらなる分析のために情報を探索することができる。これには検討中の学習アルゴリズムに関するパフォーマンスや予測などが含まれる。

### 学習器の性能

先程のベンチマーク試験の結果を見てみよう。`getBMRPerformances`は個々のリサンプリング毎の性能指標を返し、`getMBRAggrPerformances`は性能指標の集約値を返す。

```{r}
getBMRPerformances(bmr)
```

```{r}
getBMRAggrPerformances(bmr)
```

今回の例ではリサンプリング手法にホールドアウト法を選んだので、リサンプリングは1回しか行っていない。そのため、個々のリサンプリング結果に基づく性能指標と集約値はどちらも同じ表示結果になっている。

デフォルトでは、ほぼすべてのゲッター関数ネストされたリストを返す。リストの最初のレベルはタスクで分類されており、二番目のレベルは学習器での分類になる。学習器またはタスクが1つしかない場合は、`drop = TRUE`を指定するとフラットなリストを得ることもできる。

```{r}
getBMRPerformances(bmr, drop = TRUE)
```

大抵の場合はデータフレームの方が便利だろう。`as.df = TRUE`を指定すると結果をデータフレームに変換できる。

```{r}
getBMRPerformances(bmr, as.df = TRUE)
```

### 予測

デフォルトでは、`BenchmarkResult`は学習器の予測結果も含んでいる。もし、メモリ節約などの目的でこれを止めさせたければ`keep.pred = FALSE`を`benchmark`関数に指定すれば良い。

予測へのアクセスは`getBMRPredictions`関数を使う。デフォルトでは、`ResamplePrediction`オブジェクトのネストされたリストが返ってくる。性能指標の場合と同様に、ここでも`drop`及び`as.df`引数を使うことができる。

```{r}
getBMRPredictions(bmr)
```

```{r}
head(getBMRPredictions(bmr, as.df = TRUE))
```

IDを通じて特定の学習器やタスクの結果にアクセスすることもできる。多くのゲッター関数はIDを指定するための`learner.ids`引数と`task.ids`引数が用意されている。

```{r}
head(getBMRPredictions(bmr, learner.ids = "classif.rpart", as.df = TRUE))
```

デフォルトのIDが嫌なら、`makeLearner`や`make*Task`関数の`id`引数を通じて設定できる。さらに、学習器のIDを簡単に変更するための関数として`setLearnerId`関数も用意されている。

### ID

ベンチマーク試験における学習器、タスク、性能指標のIDは、以下のように取得できる。

```{r}
getBMRTaskIds(bmr)
```

```{r}
getBMRLearnerIds(bmr)
```

```{r}
getBMRMeasureIds(bmr)
```

### フィット済みモデル

デフォルトでは`BenchmarkResult`オブジェクトはフィット済みモデルも含んでいる。これは、`benchmark`関数を呼び出す際に`models = FALSE`を指定することで抑制できる。フィット済みモデルは`getBMRModels`関数を使うことで確認できる。この関数が返すのは(おそらくネストされた)`WrappedModel`オブジェクトのリストである。

```{r}
getBMRModels(bmr)
```

### 学習器と性能指標

使用された学習器は`getBMRLearners`で、性能指標は`getBMRMeasures`でそれぞれ抽出できる。

```{r}
getBMRLearners(bmr)
```

```{r}
getBMRMeasures(bmr)
```

## ベンチマーク結果のマージ

ベンチマーク試験が終わった後で、他の学習器やタスクを追加したくなったらどうすればよいだろうか。このような場合は、`mergeBenchmarkResults`関数を使えば、複数の`BenchmarkResutl`オブジェクトをマージすることができる。

先程行った線形判別分析と決定木のベンチマーク試験結果に対し、ランダムフォレストと二次判別分析の結果を追加してみよう。

まず、ランダムフォレストと二次判別分析のベンチマーク試験を行う。

```{r}
lrns2 = list(makeLearner("classif.randomForest"), makeLearner("classif.qda"))
bmr2 = benchmark(lrns2, sonar.task, rdesc, show.info = FALSE)
bmr2
```

次に、`bmr`と`bmr2`をマージする。`BenchmarkResult`オブジェクトはリストで渡す。

```{r}
mergeBenchmarkResults(list(bmr, bmr2))
```

上記の例ではリサンプルdescriptionを`benchmark`関数に指定していることに注意してもらいたい。つまり、`lda`と`rpart`は`randamForest`および`qda`とは異なる訓練/テストセットを用いて評価された可能性が高い。

異なる訓練/テストセットを用いて評価を行うと、学習器間の正確な性能比較が難しくなる。もし、他の学習器を使った試験を後から追加する見込みがあるのなら、最初から`ResampleInstances`を使用したほうが良い。リサンプルをインスタンス化しておけば、同じ訓練/テストセットを後の試験でも使用することができるからだ。

あるいは、`BenchmarkResult`オブジェクトから`ResampleInstance`を抽出し、これを他の試験に使用しても良い。例を示そう。

```{r}
# インスタンスの抽出
rin = getBMRPredictions(bmr)[[1]][[1]]$instance

# インスタンスを用いてベンチマーク試験を行う
bmr3 = benchmark(lrns2, sonar.task, rin, show.info = FALSE)

# 結果をマージする
mergeBenchmarkResults(list(bmr, bmr3))
```

## ベンチマークの分析と可視化

`mlr`はベンチマーク試験を分析する機能も備えている。これには、可視化、アルゴリズムの順位付け、パフォーマンスの差に対する仮説検定が含まれる。

今回はこの機能を紹介するために、5つの分類タスクと3つの学習アルゴリズムを使用する、やや大きなベンチマーク試験を実施する。

### 例：線形判別分析と分類木、ランダムフォレストの比較

**3つのアルゴリズム**は線形判別分析、決定木、ランダムフォレストである。これらのデフォルトの学習器IDはやや長いので、以下の例ではもう少し短い別名を設定した。

**5つの分類タスク**と言ったが、3つは既に紹介したものだ。これに加えて、`mlbench`パッケージに含まれるデータを`convertMLBenchObjToTask`関数でタスクに変換したものを用いる。

全てのタスクは10分割クロスバリデーションによりリサンプリングを行う。これは、一つのresample descriptionを`benchmark`関数に渡すことで行う。これにより、それぞれのタスクに対するリサンプリングのインスタンス化が自動的に実行される。この方法では、一つのタスク内で全ての学習器に対して同じインスタンスが使用されることになる。

タスクの数と同じ長さのリストでリサンプリング手法を指定すれば、それぞれのタスクに異なるリサンプリング手法を適用することもできる。この際渡すのはresample descriptionsでも良いし、インスタンスでも良い。

評価手法は平均誤分類率を主とするが、合わせてbalanced error rate(ber)と訓練時間(timetrain)も算出する。

```{r}
## 学習器リストの作成
lrns = list(
  makeLearner("classif.lda", id = "lda"),
  makeLearner("classif.rpart", id = "rpart"),
  makeLearner("classif.randomForest", id = "randomForest")
)

## mlbenchパッケージから追加タスクを生成する
ring.task = convertMLBenchObjToTask("mlbench.ringnorm", n = 600)
wave.task = convertMLBenchObjToTask("mlbench.waveform", n = 600)

## タスクリストの作成
tasks = list(iris.task, sonar.task, pid.task, ring.task, wave.task)
## リサンプリング手法の指定
rdesc = makeResampleDesc("CV", iters = 10)
## 評価指標の指定
meas = list(mmce, ber, timetrain)
## ベンチマーク試験の実行
bmr = benchmark(lrns, tasks, rdesc, meas, show.info = FALSE)
bmr
```

iris-exampleとPimaIndiansDiabetes-exampleでは線形判別分析が優れいているが、他のタスクではランダムフォレストが良い成績を出しているようだ。一方、訓練時間を見るとランダムフォレストは他の学習器よりも長い時間がかかっている。

パフォーマンスの平均値から何らかの結論を言いたければ、値の変動も考慮に入れる必要がある。リサンプリングの繰り返し全体で値がどのように分布しているかも考慮できれば尚良い。

10分割クロスバリデーションにより、各タスクは10回評価されていることになるが、これは以下の様にして詳細を確認できる。

```{r}
perf = getBMRPerformances(bmr, as.df = TRUE)
head(perf)
```

結果を詳しく確認すると、ランダムフォレストは全てのインスタンスで分類木より優れており、線形判別分析は時間の面でほとんどどの場合も優れている事が分かる。また、線形判別分析は時々ランダムフォレストより優れた結果を出している。このサイズの試験結果ならまだ「詳しく確認する」は可能であるが、これよりももっと大きなスケールのベンチマーク試験では、このような判断はずっと難しくなる。

`mlr`には可視化と仮説検定の仕組みがあるので、より大きなスケールの試験に対してはそれを使おう。

### 可視化

プロットには`ggplot2`パッケージが使用される。したがって、要素のリネームや配色変更などのカスタマイズは容易に行える。

#### パフォーマンスの可視化

`plotBMRBoxplot`は箱ひげ図またはバイオリンプロットによって、リサンプリングの繰り返しの間に指標がどのような値をとったのかを可視化する。これはつまり`getBMRPerformances`の可視化である。

以下に箱ひげ図で`mmce`の分布を示す。

```{r}
plotBMRBoxplots(bmr, measure = mmce)
```

次にバイオリンプロットの例を示す。

```{r}
# aes, theme, element_textをggplot2::なしで呼び出すためには
# ライブラリのロードが必要
library(ggplot2) 
plotBMRBoxplots(bmr, measure = ber, style = "violin",
                pretty.names = FALSE) +
  aes(color = learner.id) +
  theme(strip.text.x = element_text(size = 6))
```

ここでは、指標として`ber`を使用するとともに、`learner.id`に基づく色の塗り分けも行っている。また、`theme`は各パネルのラベル(strip)のフォントサイズを調整している。

`pretty.names=FALSE`は何かというと、性能指標や学習器のラベルとしてidを使ってくれという指定である。これを指定しなかったときにどのような名前が使われるかは、性能指標であれば`指標名$name`、学習器では`getBMRLEarnerShortName`で確認できる。

```{r}
mmce$name
```

```{r}
getBMRLearnerShortNames(bmr)
```

プロットに関してよくある質問は、「パネルのラベルと学習器の名前はどうやったら変更できるのか」といったものだ。例えば、今見せた例で言えば、パネルのラベル名に含まれている`-example`や`mlbench.`を取り除いたり、学習機の名前を大文字の略称に変更したい、などの思ったのではないだろうか。現状、これを解決する一番簡単なやり方はfactorのlevelを書き換えてしまうことだ。例を示そう。

```{r}
plt = plotBMRBoxplots(bmr, measure = mmce)
levels(plt$data$task.id) = c("Iris", "Ringnorm", "Waveform", "Diabetes", "Sonar")
levels(plt$data$learner.id) = c("LDA", "CART", "RF")
plt
```

### 集約結果の可視化

集約された性能指標(`getBMRAggrPerformances`で得られるもの)は、`plotBMRSummary`で可視化できる。このプロットでは各タスクの性能指標集約値が同一の線上に並べられる(注: ドットプロット)。標準では、`benchmark`で指定された最初の指標がプロット対象となる。今回の例では`mmce`だ。加えて、各ポイントには縦方向に若干の誤差が加えられる。これはデータポイントが重なった場合にも識別できるようにするためだ。

```{r}
plotBMRSummary(bmr)
```

### 順位の計算と可視化

性能指標の値に加えて、順位のような相対指標があると新しい知見が得られる場合がある。

`convertBMRToRankMatrix`関数は、集約された性能指標のひとつに基づいて順位を計算する。例として`mmce`に基づくランキングを求めてみよう。

```{r}
convertBMRToRankMatrix(bmr)
```

**順位**は最高のパフォーマンスを示した、つまり`mmce`が最も低かった手法に対して小さな値が割り当てられる。今回の例ではLDAがirisとPimaIndiansDiabetesに対して最良の結果を示したが、他のタスクではランダムフォレストが優れていた事がわかる。

`plotBMRRanksAsBarChart`関数を使うと順位を可視化することができる。`pos = "tile"`を指定すると、順位・学習器・タスクの関係がヒートマップで図示される。この場合、x軸が順位、y軸がタスク、学習器の種類が色にそれぞれ割り当てられる。

```{r}
plotBMRRanksAsBarChart(bmr, pos = "tile")
```

なお、`plotBMRSummary`でもオプションに`trafo = "rank"`を指定することで、性能指標が順位に変換されるため、同様の情報を含むプロットを作成できる。

```{r}
plotBMRSummary(bmr, trafo = "rank")
```

`plotBMRRanksAsBarChart`はデフォルトでは積み上げ棒グラフを、`pos = "dodge"`オプションを指定すると通常の棒グラフを描画する。これは各種法が獲得した順位の頻度を確認する場合に適している。

```{r}
plotBMRRanksAsBarChart(bmr)
plotBMRRanksAsBarChart(bmr, pos = "dodge")
```

### 仮説検定で学習器を比較する

多くの研究者はアルゴリズムの有意性を仮説検定によって示す必要性を感じていることだろう。ベンチマーク試験の結果の比較などに適していると思われるノンパラメトリック検定として、`mlr`はOverall Friedman testとFriedman-Nemenyi post hoc testをサポートしている。

Friedman testは`stats`パッケージの`friedman.test`関数に基いており、これは学習器間に差があるかどうかを検定する。一方、Friedman-Nemenyi testは全ての学習器間の組合せについて差があるかどうかを検定する。ノンパラメトリック検定はパラメトリック検定と違い、データの分布に関する仮定を持たなくて済むという利点がある。しかしこれは、妥当な有意水準の元で優位な差を示すためには多くのデータセットが必要であるということでもある。